{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec Word2Vec et Régression Logistique\n",
    "\n",
    "Ce notebook explique en détail un pipeline complet de classification de texte utilisant le prétraitement linguistique avec SpaCy, l'embedding Word2Vec, et un modèle de régression logistique. \n",
    "\n",
    "Nous allons explorer chaque étape, expliquer les concepts mathématiques et algorithmiques, et montrer comment les données textuelles sont transformées en vecteurs numériques pour permettre l'apprentissage automatique.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **Traitement automatique du langage naturel (TALN)** : Ensemble de techniques permettant à une machine de comprendre et manipuler du texte humain.\n",
    "- **Word2Vec** : Méthode d'embedding qui transforme chaque mot en un vecteur dense, capturant des relations sémantiques et syntaxiques.\n",
    "- **Régression Logistique** : Modèle de classification supervisée qui prédit la probabilité d'appartenance à une classe.\n",
    "- **Lemmatisation** : Réduction des mots à leur forme de base (ex : \"mangeaient\" → \"manger\").\n",
    "- **Stopwords** : Mots fréquents sans valeur informative (ex : \"le\", \"et\", \"de\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et exploration des données\n",
    "\n",
    "Nous chargeons les données d'entraînement au format JSON. Chaque ligne correspond à un texte à classer. Nous vérifions la présence de valeurs manquantes dans la colonne 'description'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_ = \"V.1.0.8\"\n",
    "print(\"Version:\", version_)\n",
    "print(\"Loading training data...\")\n",
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "df = pd.DataFrame(train_data)\n",
    "print(\"Checking for problematic rows in 'description'...\")\n",
    "print(df['description'].isnull().sum(), \"rows are null in 'description'\")\n",
    "print(df['description'].head())\n",
    "df['description'] = df['description'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement linguistique avec SpaCy\n",
    "\n",
    "Nous utilisons SpaCy pour nettoyer et lemmatiser les textes. \n",
    "Le nettoyage consiste à :\n",
    "- Supprimer les balises HTML\n",
    "- Supprimer les URLs\n",
    "- Mettre en minuscules\n",
    "- Lemmatiser chaque mot\n",
    "- Supprimer les stopwords et les tokens non alphabétiques\n",
    "\n",
    "La lemmatisation permet de réduire la dimensionnalité et d'améliorer la généralisation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Spacy model...\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Cleaning and tokenizing training data...\")\n",
    "df['description'] = df['description'].fillna(\"\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\", []  # Retourne des valeurs vides si le texte est invalide\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Suppression des balises HTML\n",
    "    text = re.sub(r'http\\S+', ' ', text)  # Suppression des URLs\n",
    "    text = text.lower().strip()  # Mise en minuscules\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]  # Lemmatisation et suppression des stopwords\n",
    "    return \" \".join(tokens), tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquons la fonction de nettoyage à chaque description. \n",
    "Chaque texte est transformé en deux colonnes :\n",
    "- `Clean` : texte nettoyé et lemmatisé (pour d'autres usages)\n",
    "- `Tokens` : liste des tokens (mots) pour l'entraînement Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and tokenizing training data...\")\n",
    "results = df['description'].apply(clean_and_tokenize)\n",
    "tuples = results.apply(lambda x: x if (isinstance(x, tuple) and len(x)==2) else (\"\",[]))\n",
    "results_df = pd.DataFrame(tuples.tolist(), index=results.index, columns=['Clean', 'Tokens'])\n",
    "df[['Clean', 'Tokens']] = results_df\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding des textes avec Word2Vec\n",
    "\n",
    "**Word2Vec** est un algorithme non supervisé qui apprend à représenter chaque mot par un vecteur dense de dimension fixe (ici 100). \n",
    "L'idée est que des mots ayant des contextes similaires auront des vecteurs proches dans l'espace vectoriel.\n",
    "\n",
    "Il existe deux architectures principales :\n",
    "- **CBOW (Continuous Bag of Words)** : prédit un mot à partir de son contexte\n",
    "- **Skip-gram** : prédit le contexte à partir d'un mot (ici `sg=1` pour skip-gram)\n",
    "\n",
    "Mathématiquement, Word2Vec cherche à maximiser la probabilité de prédire les mots de contexte autour d'un mot cible, ce qui revient à factoriser la matrice de co-occurrence des mots.\n",
    "\n",
    "Après entraînement, chaque mot est représenté par un vecteur de taille 100. Pour représenter un texte, on fait la moyenne des vecteurs des mots présents dans le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model...\")\n",
    "model = Word2Vec(\n",
    "    size=100,  # Taille des vecteurs de mots\n",
    "    window=5,  # Fenêtre de contexte\n",
    "    min_count=2,  # Ignore les mots trop rares\n",
    "    workers=4,    # Nombre de threads\n",
    "    sg=1,         # Skip-gram\n",
    "    iter=10       # Nombre d'itérations\n",
    ")\n",
    "model.build_vocab(df['Tokens'])\n",
    "model.train(df['Tokens'], total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des labels (catégories)\n",
    "\n",
    "Nous associons à chaque texte sa catégorie cible pour l'apprentissage supervisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading labels...\")\n",
    "labels_df = pd.read_csv('train_label.csv')\n",
    "df = df.merge(labels_df, on='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation des textes par moyenne des vecteurs Word2Vec\n",
    "\n",
    "Pour chaque texte, on calcule la moyenne des vecteurs Word2Vec de ses mots. \n",
    "Cela donne un vecteur de taille 100 représentant le texte entier.\n",
    "\n",
    "Si un mot n'est pas dans le vocabulaire Word2Vec, il est ignoré. Si aucun mot n'est connu, on retourne un vecteur nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorizing tokens using Word2Vec...\")\n",
    "def vectorize_tokens(tokens, model):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df['W2V_Vector'] = df['Tokens'].apply(lambda tokens: vectorize_tokens(tokens, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données pour l'apprentissage supervisé\n",
    "\n",
    "On construit la matrice X (features) et le vecteur y (labels) pour entraîner le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing training and testing data...\")\n",
    "X = np.vstack(df['W2V_Vector'].values)\n",
    "y = df['Category'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle de Régression Logistique\n",
    "\n",
    "La **régression logistique** est un modèle linéaire de classification. \n",
    "Elle calcule la probabilité d'appartenance à chaque classe à partir d'une combinaison linéaire des features (ici, les vecteurs Word2Vec).\n",
    "\n",
    "Mathématiquement, pour chaque classe $k$ :\n",
    "$$ P(y=k|x) = \\frac{\\exp(w_k^T x + b_k)}{\\sum_j \\exp(w_j^T x + b_j)} $$\n",
    "où $x$ est le vecteur du texte, $w_k$ et $b_k$ sont les paramètres appris pour la classe $k$.\n",
    "\n",
    "Le modèle est entraîné pour maximiser la vraisemblance des labels sur l'ensemble d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression model...\")\n",
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation et vectorisation des données de test\n",
    "\n",
    "On applique le même prétraitement et la même vectorisation aux textes de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading test data...\")\n",
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"Cleaning and tokenizing test data...\")\n",
    "test_df[['Clean', 'Tokens']] = test_df['description'].apply(lambda t: pd.Series(clean_and_tokenize(t)))\n",
    "\n",
    "print(\"Vectorizing test data using Word2Vec...\")\n",
    "test_df['W2V_Vector'] = test_df['Tokens'].apply(lambda tokens: vectorize_tokens(tokens, model))\n",
    "X_test_w2v = np.vstack(test_df['W2V_Vector'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des catégories sur les données de test\n",
    "\n",
    "Le modèle prédit la catégorie la plus probable pour chaque texte de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting categories for test data...\")\n",
    "test_df['Predicted_Category_W2V'] = clf_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing submission file...\")\n",
    "template = pd.read_csv('template_submissions.csv')\n",
    "template['Category'] = template['Id'].map(\n",
    "    test_df.set_index('Id')['Predicted_Category_W2V']\n",
    ")\n",
    "template.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'.\")\n",
    "print(\"Version:\", version_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et concepts mathématiques\n",
    "\n",
    "- **Word2Vec** permet de transformer le texte en vecteurs denses, où la proximité géométrique reflète la similarité sémantique.\n",
    "- La **moyenne des vecteurs** pour chaque texte permet d'obtenir une représentation globale, adaptée à la classification.\n",
    "- La **régression logistique** sépare les classes dans l'espace vectoriel à l'aide d'hyperplans, en maximisant la probabilité des labels observés.\n",
    "\n",
    "Ce pipeline est un exemple classique de l'application de l'intelligence artificielle au traitement du langage naturel :\n",
    "1. Prétraitement linguistique\n",
    "2. Transformation des textes en vecteurs numériques\n",
    "3. Apprentissage supervisé pour la classification\n",
    "\n",
    "Ce type d'approche est utilisé dans la détection de spam, la modération de contenu, la recommandation de documents, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
