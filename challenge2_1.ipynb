{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement avancé pour la classification de texte\n",
    "\n",
    "Ce notebook détaille un pipeline complet de prétraitement pour la classification de texte. Il explique chaque étape, du chargement des données à la sauvegarde des matrices prêtes pour l'apprentissage automatique.\n",
    "\n",
    "Nous utilisons :\n",
    "- Nettoyage et lemmatisation avec SpaCy\n",
    "- Vectorisation avec CountVectorizer\n",
    "- Encodage des labels\n",
    "- Padding/truncation des matrices\n",
    "- Sauvegarde des artefacts pour un usage ultérieur\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version et introduction\n",
    "\n",
    "On affiche la version du script et on explique le but : préparer les données pour un modèle de classification de texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"V2_1.1.0 – Prétraitement train + test et sauvegarde\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données d'entraînement\n",
    "\n",
    "On charge les données d'entraînement depuis un fichier JSON. Chaque ligne correspond à un texte à classer. On remplace les valeurs manquantes par une chaîne vide pour éviter les erreurs lors du traitement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading training data...\")\n",
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "df = pd.DataFrame(train_data).fillna('')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion des labels d'entraînement\n",
    "\n",
    "On associe à chaque texte sa catégorie cible (label) pour l'apprentissage supervisé."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Merging train labels...\")\n",
    "labels_df = pd.read_csv('train_label.csv')\n",
    "df = df.merge(labels_df, on='Id')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données de test\n",
    "\n",
    "On charge les textes à prédire (test) et on remplace aussi les valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading test data...\")\n",
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data).fillna('')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nettoyage et lemmatisation des textes\n",
    "\n",
    "On utilise SpaCy pour nettoyer et lemmatiser les textes :\n",
    "- Suppression du HTML et des URLs\n",
    "- Mise en minuscules\n",
    "- Lemmatisation (réduction à la racine)\n",
    "- Suppression des stopwords et tokens non alphabétiques\n",
    "\n",
    "Ce prétraitement réduit la dimensionnalité et améliore la qualité des features pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Cleaning and lemmatizing text data...\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    doc = nlp(text)\n",
    "    return ' '.join(tok.lemma_ for tok in doc if tok.is_alpha and not tok.is_stop)\n",
    "\n",
    "df['Clean']      = df['description'].apply(clean_text)\n",
    "test_df['Clean'] = test_df['description'].apply(clean_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encodage des labels d'entraînement\n",
    "\n",
    "On transforme les catégories textuelles en entiers avec `LabelEncoder`. Cela permet de les utiliser comme cibles pour l'apprentissage automatique.\n",
    "\n",
    "On prépare aussi le nombre de classes pour le one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Encoding train labels...\")\n",
    "le = LabelEncoder()\n",
    "df['y'] = le.fit_transform(df['Category'])\n",
    "num_classes = len(le.classes_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorisation des textes (CountVectorizer)\n",
    "\n",
    "On transforme les textes en vecteurs de nombres avec `CountVectorizer` :\n",
    "- Chaque texte devient un vecteur de taille `max_features` (ici 20 000)\n",
    "- Chaque dimension correspond à un mot du vocabulaire\n",
    "- La valeur est le nombre d'occurrences du mot dans le texte\n",
    "\n",
    "Mathématiquement, cela revient à construire une matrice creuse (sparse) de taille (n_samples, vocab_size), où chaque ligne représente un texte et chaque colonne un mot du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Vectorizing train + test (CountVectorizer)...\")\n",
    "MAX_VOCAB = 20000\n",
    "MAX_LEN   = 100\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_VOCAB, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(df['Clean']).toarray()\n",
    "X_test  = vectorizer.transform(test_df['Clean']).toarray()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Padding/Truncation des matrices\n",
    "\n",
    "Pour garantir que tous les vecteurs aient la même taille (`MAX_LEN`), on tronque ou complète les vecteurs :\n",
    "- Si le vecteur est trop long, on coupe\n",
    "- S'il est trop court, on complète avec des zéros\n",
    "\n",
    "Cela permet d'utiliser ces matrices comme entrée pour des modèles de deep learning ou d'autres algorithmes qui exigent une taille fixe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def pad_trunc(mat, max_len):\n",
    "    if mat.shape[1] > max_len:\n",
    "        return mat[:, :max_len]\n",
    "    elif mat.shape[1] < max_len:\n",
    "        return np.pad(mat, ((0,0),(0, max_len - mat.shape[1])), mode='constant')\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "X_train = pad_trunc(X_train, MAX_LEN)\n",
    "X_test  = pad_trunc(X_test,  MAX_LEN)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. One-hot encoding des labels d'entraînement\n",
    "\n",
    "On transforme les labels en vecteurs one-hot :\n",
    "- Pour chaque exemple, le vecteur est de taille `num_classes`\n",
    "- La case correspondant à la classe est à 1, les autres à 0\n",
    "\n",
    "Cela facilite l'entraînement de modèles multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_train = np.eye(num_classes)[df['y']]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des artefacts\n",
    "\n",
    "On sauvegarde :\n",
    "- Le vectorizer (pour transformer les nouveaux textes de la même façon)\n",
    "- Le label encoder (pour décoder les prédictions)\n",
    "- Les matrices numpy prêtes pour l'entraînement ou l'inférence\n",
    "\n",
    "Cela permet de réutiliser exactement le même prétraitement lors de la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Saving vectorizer, label encoder, X_train.npy, y_train.npy, X_test.npy …\")\n",
    "joblib.dump(vectorizer,    'vectorizer.joblib')\n",
    "joblib.dump(le,            'label_encoder.joblib')\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('X_test.npy',  X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du pipeline\n",
    "\n",
    "Le prétraitement est terminé. Les fichiers sauvegardés peuvent être utilisés pour entraîner un modèle de classification de texte (réseau de neurones, SVM, etc.).\n",
    "\n",
    "Ce pipeline garantit que le texte brut est transformé en vecteurs numériques exploitables par les algorithmes d'apprentissage automatique, tout en conservant la correspondance entre les textes et leurs catégories."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"✅ Prétraitement terminé et fichiers sauvegardés.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}