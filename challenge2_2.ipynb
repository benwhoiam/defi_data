{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec un MLP PyTorch sur Bag-of-Words\n",
    "\n",
    "Ce notebook explique en détail comment entraîner un perceptron multicouche (MLP) avec PyTorch pour la classification de texte, à partir de représentations bag-of-words prétraitées. \n",
    "Chaque étape est expliquée, du chargement des données à la génération du fichier de soumission.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **Bag-of-Words (BOW)** : Représentation vectorielle d'un texte où chaque dimension correspond à un mot du vocabulaire, et la valeur est le nombre d'occurrences du mot dans le texte.\n",
    "- **MLP (Multi-Layer Perceptron)** : Réseau de neurones à couches entièrement connectées, capable de modéliser des relations non linéaires.\n",
    "- **CrossEntropyLoss** : Fonction de perte adaptée à la classification multi-classes, qui mesure la différence entre la distribution prédite et la vraie classe.\n",
    "- **Adam** : Optimiseur efficace pour l'entraînement des réseaux de neurones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version et introduction\n",
    "\n",
    "On affiche la version du script et on explique le but : entraîner un modèle de classification de texte avec PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"V2_1.3.1 – Entraînement + Prédiction PyTorch corrigé\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des artefacts prétraités\n",
    "\n",
    "On charge les matrices numpy contenant les représentations bag-of-words (BOW) des textes d'entraînement et de test, ainsi que les labels encodés en one-hot. \n",
    "On charge aussi le vectorizer et le label encoder pour décoder les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train = np.load('X_train.npy')        # représentation bag-of-words pad/trunc\n",
    "y_train_onehot = np.load('y_train.npy') # one-hot\n",
    "X_test = np.load('X_test.npy')          # représentation bag-of-words pad/trunc\n",
    "\n",
    "vectorizer = joblib.load('vectorizer.joblib')     # transformateur BOW\n",
    "le = joblib.load('label_encoder.joblib')           # encodeur de labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Préparation des labels pour CrossEntropyLoss\n",
    "\n",
    "La fonction de perte `CrossEntropyLoss` attend des indices de classes (entiers), pas des vecteurs one-hot. On convertit donc les labels one-hot en indices avec `np.argmax`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_train = np.argmax(y_train_onehot, axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conversion en tensors PyTorch\n",
    "\n",
    "On convertit les matrices numpy en tensors PyTorch pour pouvoir les utiliser dans le DataLoader et le modèle. \n",
    "On crée un DataLoader pour itérer sur les données par mini-batchs lors de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dtype = torch.float32\n",
    "X_tr = torch.tensor(X_train, dtype=dtype)\n",
    "y_tr = torch.tensor(y_train, dtype=torch.long)\n",
    "X_te = torch.tensor(X_test, dtype=dtype)\n",
    "\n",
    "dataset = TensorDataset(X_tr, y_tr)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Définition du modèle MLP pour BOW\n",
    "\n",
    "On définit un perceptron multicouche (MLP) simple :\n",
    "- Une couche linéaire (entrée → cachée)\n",
    "- Activation ReLU (introduit de la non-linéarité)\n",
    "- Dropout (régularisation pour éviter le surapprentissage)\n",
    "- Une couche linéaire (cachée → sortie)\n",
    "\n",
    "Mathématiquement, le MLP calcule :\n",
    "$$ y = W_2 (\\text{ReLU}(W_1 x + b_1)) + b_2 $$\n",
    "où $x$ est le vecteur BOW, $W_1$, $W_2$ sont les matrices de poids, $b_1$, $b_2$ les biais."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]  # longueur du vecteur BOW\n",
    "hidden_dim = 64\n",
    "num_classes = y_train_onehot.shape[1]\n",
    "model = SimpleMLP(input_dim, hidden_dim, num_classes)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Critère et optimiseur\n",
    "\n",
    "- **CrossEntropyLoss** : combine un softmax et la log-vraisemblance pour la classification multi-classes.\n",
    "- **Adam** : optimiseur adaptatif efficace pour l'entraînement des réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle\n",
    "\n",
    "On entraîne le modèle pendant plusieurs époques (passes sur toutes les données). À chaque batch :\n",
    "- On calcule les logits (sorties non normalisées du modèle)\n",
    "- On calcule la perte\n",
    "- On rétro-propage le gradient et on met à jour les poids\n",
    "\n",
    "La perte moyenne par batch est affichée à chaque époque pour suivre la convergence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "epochs = 15\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{epochs} – Loss: {total_loss/len(train_loader):.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prédiction sur les données de test\n",
    "\n",
    "On met le modèle en mode évaluation, on calcule les logits pour chaque texte de test, puis on prend la classe avec la probabilité la plus élevée (`argmax`).\n",
    "On utilise le label encoder pour retrouver les noms de catégories d'origine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_te)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    pred_labels = le.inverse_transform(preds)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Saving submission2_2.csv...\")\n",
    "# Charger les Id du test\n",
    "test_ids = pd.read_json('test.json', orient='records')['Id']\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'Category': pred_labels\n",
    "})\n",
    "submission.to_csv('submission2_2.csv', index=False)\n",
    "print(\"✅ submission2_2.csv généré.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **Bag-of-Words** : chaque texte est représenté par un vecteur de dimension $d$ (taille du vocabulaire ou tronquée), où chaque entrée $x_i$ est le nombre d'occurrences du mot $i$ dans le texte.\n",
    "- **MLP** : le réseau apprend une fonction $f(x) = \\text{softmax}(W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2)$ qui approxime la probabilité d'appartenance à chaque classe.\n",
    "- **CrossEntropyLoss** : pour chaque exemple, la perte est $-\\log(p_{y})$ où $p_{y}$ est la probabilité prédite pour la vraie classe.\n",
    "\n",
    "Ce pipeline montre comment passer de textes bruts à des prédictions de classes avec un réseau de neurones, en utilisant des représentations vectorielles simples mais efficaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}