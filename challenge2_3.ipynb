{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec LSTM sous PyTorch\n",
    "\n",
    "Ce notebook détaille l'entraînement et la prédiction d'un modèle LSTM (Long Short-Term Memory) pour la classification de texte. \n",
    "Chaque étape est expliquée, du chargement des données séquentielles à la génération du fichier de soumission.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **LSTM** : Réseau de neurones récurrent (RNN) conçu pour traiter des séquences et mémoriser des dépendances à long terme. Idéal pour le texte.\n",
    "- **Embedding** : Transformation des indices de mots en vecteurs denses, apprises pendant l'entraînement.\n",
    "- **Pondération des classes** : Correction du déséquilibre des classes en pondérant la perte.\n",
    "- **CrossEntropyLoss** : Fonction de perte adaptée à la classification multi-classes.\n",
    "- **Adam** : Optimiseur efficace pour l'entraînement des réseaux de neurones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version et introduction\n",
    "\n",
    "On affiche la version du script et on explique le but : entraîner un modèle LSTM pour la classification de texte séquentiel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"V2_1.2.1 – Entraînement + Prédiction LSTM PyTorch\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des artefacts séquentiels\n",
    "\n",
    "On charge les matrices numpy contenant les séquences d'indices de mots (`X_train_seq`, `X_test_seq`), les labels, le vocabulaire (mapping mot→indice) et l'encodeur de labels.\n",
    "\n",
    "- `X_train_seq` et `X_test_seq` : matrices (n_samples, max_len) où chaque ligne est une séquence d'indices de mots.\n",
    "- `stoi` : dictionnaire mot→indice, utilisé pour l'embedding.\n",
    "- `le` : encodeur pour transformer les labels en entiers et inversement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train = np.load('X_train_seq.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_test  = np.load('X_test_seq.npy')\n",
    "stoi     = joblib.load('vocab_stoi.pkl')\n",
    "le       = joblib.load('label_encoder.pkl')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversion en tensors PyTorch\n",
    "\n",
    "On convertit les matrices numpy en tensors PyTorch pour pouvoir les utiliser dans le DataLoader et le modèle. \n",
    "On crée un DataLoader pour itérer sur les données par mini-batchs lors de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_tr = torch.tensor(X_train, dtype=torch.long)\n",
    "y_tr = torch.tensor(y_train, dtype=torch.long)\n",
    "X_te = torch.tensor(X_test,  dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_tr, y_tr)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du modèle LSTM\n",
    "\n",
    "Le modèle comprend :\n",
    "- Une couche d'embedding (transforme chaque indice de mot en vecteur dense)\n",
    "- Un LSTM (traite la séquence de vecteurs et capture les dépendances contextuelles)\n",
    "- Un dropout (régularisation)\n",
    "- Une couche linéaire finale pour la classification\n",
    "\n",
    "Mathématiquement, le LSTM apprend à mémoriser les informations pertinentes dans la séquence grâce à ses cellules mémoire et ses portes (input, forget, output). La sortie finale (dernier état caché) est utilisée pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm      = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout   = nn.Dropout(0.3)\n",
    "        self.fc        = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb, _ = self.lstm(self.embedding(x))\n",
    "        last = emb[:, -1, :]\n",
    "        return self.fc(self.dropout(last))\n",
    "\n",
    "vocab_size  = len(stoi)\n",
    "embed_dim   = 128\n",
    "hidden_dim  = 64\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, pad_idx=stoi['<PAD>'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration de l’entraînement et pondération des classes\n",
    "\n",
    "Pour corriger le déséquilibre des classes, on calcule des poids inverses à la fréquence de chaque classe et on les passe à la fonction de perte `CrossEntropyLoss`.\n",
    "\n",
    "- **CrossEntropyLoss** : combine softmax et log-vraisemblance pour la classification multi-classes.\n",
    "- **Adam** : optimiseur adaptatif efficace pour l'entraînement des réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calcul manuel des poids de classes\n",
    "counts = np.bincount(y_train, minlength=num_classes)\n",
    "total  = y_train.shape[0]\n",
    "counts = np.where(counts == 0, 1, counts)  # éviter division par zéro\n",
    "class_weights = torch.tensor((total / counts), dtype=torch.float)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs    = 15"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle\n",
    "\n",
    "À chaque époque, pour chaque batch :\n",
    "- On calcule les logits (sorties non normalisées du modèle)\n",
    "- On calcule la perte pondérée\n",
    "- On rétro-propage le gradient et on met à jour les poids\n",
    "- On mesure la précision sur le batch\n",
    "\n",
    "La perte et la précision moyennes sont affichées à chaque époque pour suivre la convergence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total   += yb.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc  = correct / total\n",
    "    print(f\"Epoch {epoch}/{epochs} — loss: {train_loss:.4f} — acc: {train_acc:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prédiction sur le test\n",
    "\n",
    "On met le modèle en mode évaluation, on calcule les logits pour chaque séquence de test, puis on prend la classe avec la probabilité la plus élevée (`argmax`).\n",
    "On utilise le label encoder pour retrouver les noms de catégories d'origine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_te)\n",
    "    preds  = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    cats   = le.inverse_transform(preds)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Saving submission2_3.csv...\")\n",
    "test_ids   = pd.read_json('test_mini.json', orient='records')['Id']\n",
    "submission = pd.DataFrame({'Id': test_ids, 'Category': cats})\n",
    "submission.to_csv('submission2_3.csv', index=True)\n",
    "print(\"✅ submission2_3.csv généré.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **Embedding** : chaque mot est représenté par un vecteur dense appris pendant l'entraînement. L'embedding permet de capturer la similarité sémantique entre mots.\n",
    "- **LSTM** : pour chaque séquence $x_1, ..., x_T$, le LSTM produit une séquence d'états cachés $h_t$ qui résument l'information du passé. Le dernier état caché est utilisé pour la classification.\n",
    "- **CrossEntropyLoss** : pour chaque exemple, la perte est $-\\log(p_{y})$ où $p_{y}$ est la probabilité prédite pour la vraie classe.\n",
    "\n",
    "Ce pipeline montre comment passer de séquences de mots à des prédictions de classes avec un réseau LSTM, en tenant compte du contexte et de l'ordre des mots dans le texte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}