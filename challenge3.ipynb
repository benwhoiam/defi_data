{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec un réseau de neurones Keras\n",
    "\n",
    "Ce notebook détaille un pipeline complet de classification de texte avec Keras/TensorFlow, du prétraitement à la prédiction. Chaque étape est expliquée, y compris les principes mathématiques et les choix algorithmiques.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **Prétraitement linguistique** : Nettoyage, lemmatisation, suppression des stopwords pour améliorer la qualité des features.\n",
    "- **Tokenization & Padding** : Transformation du texte en séquences d'indices, puis normalisation de la longueur des séquences.\n",
    "- **Embedding** : Apprentissage de représentations vectorielles denses pour chaque mot.\n",
    "- **GlobalAveragePooling1D** : Réduction d'une séquence de vecteurs en un seul vecteur par moyenne.\n",
    "- **Dense & Dropout** : Couches du réseau de neurones pour la classification et la régularisation.\n",
    "- **Softmax** : Fonction d'activation pour la classification multi-classes.\n",
    "- **Categorical Crossentropy** : Fonction de perte adaptée à la classification multi-classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement du texte\n",
    "\n",
    "On utilise SpaCy pour nettoyer et lemmatiser les textes :\n",
    "- Suppression du HTML et des URLs\n",
    "- Mise en minuscules\n",
    "- Lemmatisation (réduction à la racine)\n",
    "- Suppression des stopwords et tokens non alphabétiques\n",
    "\n",
    "Ce prétraitement réduit la dimensionnalité et améliore la qualité des features pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading SpaCy model...\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du jeu de données et nettoyage\n",
    "\n",
    "On charge les données d'entraînement, puis on applique le nettoyage et la lemmatisation à chaque description."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "version_ = \"V.3.0.6\"\n",
    "print(\"Version:\", version_)\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_json('train_mini.json').set_index('Id')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "data['Cleaned_Description'] = data['description'].apply(clean_and_tokenize)\n",
    "X = data['Cleaned_Description']\n",
    "print(f\"Number of samples: {len(X)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping des descriptions vers les catégories\n",
    "\n",
    "On utilise un fichier de mapping pour associer chaque description nettoyée à une catégorie métier. Cela permet de transformer le problème en classification supervisée."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading job categories...\")\n",
    "categories_mapping = pd.read_csv('categories_string.csv', header=None, index_col=0, squeeze=\"columns\").to_dict()\n",
    "\n",
    "print(\"Mapping descriptions to job categories...\")\n",
    "data['Category'] = data['Cleaned_Description'].map(categories_mapping)\n",
    "\n",
    "unmapped = data[data['Category'].isnull()]\n",
    "if not unmapped.empty:\n",
    "    print(\"Warning: Some descriptions could not be mapped. Unmapped values:\")\n",
    "    print(unmapped[['description', 'Cleaned_Description']])\n",
    "\n",
    "data = data.dropna(subset=['Category'])\n",
    "if data.empty:\n",
    "    raise ValueError(\"No valid data left after mapping categories. Check your input files.\")\n",
    "print(\"Categories mapped successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage des catégories (labels)\n",
    "\n",
    "On transforme les catégories textuelles en entiers avec `LabelEncoder`, puis en one-hot avec `to_categorical` pour la classification multi-classes.\n",
    "\n",
    "Mathématiquement, chaque label $y$ est transformé en un vecteur de taille $C$ (nombre de classes), où la case correspondant à la classe est à 1, les autres à 0."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Encoding job categories...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['Category'])\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "print(\"Job categories encoded successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et padding des textes\n",
    "\n",
    "On transforme chaque texte en une séquence d'indices de mots (`Tokenizer`), puis on tronque ou complète chaque séquence à une longueur fixe (`pad_sequences`).\n",
    "\n",
    "Cela permet d'obtenir une matrice $(n_{samples}, maxlen)$ où chaque ligne représente un texte sous forme d'indices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Tokenizing and padding text data...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=100)\n",
    "print(\"Text data tokenized and padded successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle de réseau de neurones\n",
    "\n",
    "Le modèle comprend :\n",
    "- Une couche d'embedding (chaque mot est représenté par un vecteur dense appris pendant l'entraînement)\n",
    "- Un GlobalAveragePooling1D (fait la moyenne des vecteurs de mots pour obtenir une représentation globale du texte)\n",
    "- Une couche dense cachée avec activation ReLU\n",
    "- Un dropout (régularisation)\n",
    "- Une couche de sortie softmax (probabilité pour chaque classe)\n",
    "\n",
    "Mathématiquement, le modèle apprend une fonction $f(x) = \\text{softmax}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot \\text{mean}(E(x)) + b_1) + b_2)$ où $E(x)$ est la séquence des embeddings du texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Building the neural network model...\")\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "print(\"Model built successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation du modèle\n",
    "\n",
    "- **Adam** : optimiseur adaptatif efficace pour l'entraînement des réseaux de neurones.\n",
    "- **Categorical Crossentropy** : fonction de perte adaptée à la classification multi-classes.\n",
    "- **Accuracy** : métrique d'évaluation (proportion de bonnes prédictions)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Model compiled successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle\n",
    "\n",
    "On entraîne le modèle sur toutes les données disponibles. À chaque époque, le modèle ajuste ses poids pour minimiser la perte.\n",
    "\n",
    "La descente de gradient ajuste les poids pour maximiser la probabilité des bonnes classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Training the model on all available data...\")\n",
    "model.fit(X_padded, y_categorical, epochs=10, batch_size=32, verbose=1)\n",
    "print(\"Model training completed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation et prétraitement des données de test\n",
    "\n",
    "On applique le même nettoyage, tokenization et padding aux textes de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading test dataset...\")\n",
    "test_data = pd.read_json('test_mini.json').set_index('Id')\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_data['Cleaned_Description'] = test_data['description'].apply(clean_and_tokenize)\n",
    "X_test = test_data['Cleaned_Description']\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_tokenized, maxlen=100)\n",
    "print(\"Test data preprocessed successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction sur les données de test\n",
    "\n",
    "Le modèle prédit la probabilité d'appartenance à chaque classe pour chaque texte. On prend la classe avec la probabilité maximale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Generating predictions for test data...\")\n",
    "predictions = model.predict(X_test_padded)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "print(\"Predictions generated successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décodage des prédictions\n",
    "\n",
    "On utilise le label encoder pour retrouver les noms de catégories d'origine à partir des indices prédits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Mapping predictions to job categories...\")\n",
    "predicted_categories = label_encoder.inverse_transform(predicted_classes)\n",
    "print(\"Predictions mapped to job categories successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Saving predictions to submissions3.csv...\")\n",
    "submissions3 = pd.DataFrame({\n",
    "    'Id': test_data.index,\n",
    "    'Category': predicted_categories\n",
    "})\n",
    "submissions3.to_csv('submissions3.csv', index=False)\n",
    "print(\"Predictions saved to submissions3.csv successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **Embedding** : chaque mot est représenté par un vecteur dense appris pendant l'entraînement. L'embedding permet de capturer la similarité sémantique entre mots.\n",
    "- **GlobalAveragePooling1D** : fait la moyenne des embeddings pour obtenir une représentation globale du texte.\n",
    "- **Dense & Softmax** : la couche dense calcule une combinaison linéaire des features, softmax transforme ces scores en probabilités.\n",
    "- **Categorical Crossentropy** : pour chaque exemple, la perte est $-\\sum_{c} y_c \\log(p_c)$ où $y_c$ est la vraie classe (one-hot) et $p_c$ la probabilité prédite.\n",
    "\n",
    "Ce pipeline montre comment passer de textes bruts à des prédictions de classes avec un réseau de neurones, en utilisant des représentations vectorielles apprises automatiquement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}