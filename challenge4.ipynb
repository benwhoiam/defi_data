{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec MLP et embeddings SpaCy\n",
    "\n",
    "Ce notebook détaille un pipeline complet de classification de texte utilisant un réseau de neurones (MLP) et les embeddings pré-entraînés de SpaCy. Chaque étape est expliquée, du prétraitement à la génération du fichier de soumission.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **Embeddings SpaCy** : chaque mot est représenté par un vecteur dense issu d'un modèle pré-entraîné, capturant la sémantique du mot.\n",
    "- **Moyenne des embeddings** : pour chaque texte, on fait la moyenne des vecteurs des mots pour obtenir une représentation globale.\n",
    "- **MLPClassifier** : réseau de neurones à couches entièrement connectées (Multi-Layer Perceptron) pour la classification.\n",
    "- **Prétraitement linguistique** : nettoyage, lemmatisation, suppression des stopwords pour améliorer la qualité des features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import json"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version et introduction\n",
    "\n",
    "On affiche la version du script et on explique le but : entraîner un MLP sur des embeddings SpaCy pour la classification de texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "version_ = \"V.4.0.2\"\n",
    "print(\"Version:\", version_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du modèle SpaCy pré-entraîné\n",
    "\n",
    "On utilise le modèle SpaCy pour lemmatiser et vectoriser les mots. Les embeddings sont appris sur de grands corpus et capturent la similarité sémantique entre mots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading pre-trained SpaCy model...\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et nettoyage des données d'entraînement\n",
    "\n",
    "On charge les données d'entraînement et on remplace les valeurs manquantes dans la colonne 'description' par une chaîne vide."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading training data...\")\n",
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "df = pd.DataFrame(train_data)\n",
    "print(\"Checking for problematic rows in 'description'...\")\n",
    "df['description'] = df['description'].fillna(\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage et lemmatisation des textes\n",
    "\n",
    "On définit une fonction qui :\n",
    "- enlève le HTML et les URLs\n",
    "- met en minuscules\n",
    "- lemmatise et enlève les stopwords et tokens non alphabétiques\n",
    "\n",
    "On retourne à la fois le texte nettoyé et la liste des tokens pour la vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\", []\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens), tokens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à chaque description pour obtenir deux colonnes :\n",
    "- `Clean` : texte nettoyé et lemmatisé\n",
    "- `Tokens` : liste des tokens pour la vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Cleaning and tokenizing training data...\")\n",
    "results = df['description'].apply(clean_and_tokenize)\n",
    "tuples = results.apply(lambda x: x if (isinstance(x, tuple) and len(x) == 2) else (\"\", []))\n",
    "results_df = pd.DataFrame(tuples.tolist(), index=results.index, columns=['Clean', 'Tokens'])\n",
    "df[['Clean', 'Tokens']] = results_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des labels\n",
    "\n",
    "On associe à chaque texte sa catégorie cible pour l'apprentissage supervisé."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading labels...\")\n",
    "labels_df = pd.read_csv('train_label.csv')\n",
    "df = df.merge(labels_df, on='Id')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation des textes par moyenne des embeddings SpaCy\n",
    "\n",
    "Pour chaque texte, on calcule la moyenne des vecteurs SpaCy de ses mots. Cela donne un vecteur de taille fixe représentant le texte entier.\n",
    "\n",
    "Si un mot n'a pas de vecteur, il est ignoré. Si aucun mot n'est connu, on retourne un vecteur nul."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def vectorize_tokens_spacy(tokens, nlp_model):\n",
    "    vectors = [nlp_model(token).vector for token in tokens if token.strip()]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(nlp_model(\"word\").vector.shape[0])\n",
    "\n",
    "print(\"Vectorizing tokens using SpaCy's pre-trained model...\")\n",
    "df['Vector'] = df['Tokens'].apply(lambda tokens: vectorize_tokens_spacy(tokens, nlp))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données pour l'apprentissage supervisé\n",
    "\n",
    "On construit la matrice X (features) et le vecteur y (labels) pour entraîner le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Preparing training data...\")\n",
    "X = np.vstack(df['Vector'].values)\n",
    "y = df['Category'].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle de réseau de neurones (MLP)\n",
    "\n",
    "On utilise un MLP (Multi-Layer Perceptron) avec deux couches cachées. Le MLP apprend à séparer les classes dans l'espace vectoriel des textes.\n",
    "\n",
    "Mathématiquement, le MLP apprend une fonction $f(x) = W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2$ où $x$ est le vecteur du texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Training Neural Network model...\")\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
    "nn_model.fit(X, y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement des données de test\n",
    "\n",
    "On applique le même nettoyage et la même vectorisation aux textes de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading test data...\")\n",
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"Cleaning and tokenizing test data...\")\n",
    "test_df[['Clean', 'Tokens']] = test_df['description'].apply(lambda t: pd.Series(clean_and_tokenize(t)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation des textes de test\n",
    "\n",
    "On calcule la moyenne des embeddings SpaCy pour chaque texte de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Vectorizing test data using SpaCy's pre-trained model...\")\n",
    "test_df['Vector'] = test_df['Tokens'].apply(lambda tokens: vectorize_tokens_spacy(tokens, nlp))\n",
    "X_test = np.vstack(test_df['Vector'].values)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des catégories sur les données de test\n",
    "\n",
    "Le modèle prédit la catégorie la plus probable pour chaque texte de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Predicting categories for test data...\")\n",
    "test_df['Predicted_Category_NN'] = nn_model.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Preparing submission file...\")\n",
    "template = pd.read_csv('template_submissions.csv')\n",
    "template['Category'] = template['Id'].map(\n",
    "    test_df.set_index('Id')['Predicted_Category_NN']\n",
    ")\n",
    "template.to_csv('submission4.csv', index=False)\n",
    "print(\"Submission file saved as 'submission4.csv'.\")\n",
    "print(\"Version:\", version_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **Embeddings SpaCy** : chaque mot est représenté par un vecteur dense appris sur de grands corpus. La moyenne des embeddings donne une représentation globale du texte.\n",
    "- **MLP** : le réseau apprend une fonction non linéaire pour séparer les classes dans l'espace vectoriel.\n",
    "- **Classification** : la sortie du MLP est une probabilité pour chaque classe, la classe avec la plus grande probabilité est choisie.\n",
    "\n",
    "Ce pipeline montre comment passer de textes bruts à des prédictions de classes avec un réseau de neurones, en utilisant des représentations vectorielles avancées."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}