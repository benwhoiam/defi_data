{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec TF-IDF et SVM linéaire\n",
    "\n",
    "Ce notebook détaille un pipeline complet de classification de texte utilisant le prétraitement linguistique avec SpaCy, la vectorisation TF-IDF et un classifieur SVM linéaire (LinearSVC). Chaque étape est expliquée, du nettoyage à la génération du fichier de soumission.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)** : transforme chaque texte en un vecteur de nombres, reflétant l'importance des mots dans le corpus.\n",
    "- **SVM linéaire (Support Vector Machine)** : algorithme de classification supervisée qui cherche l'hyperplan séparant au mieux les classes dans l'espace vectoriel.\n",
    "- **Prétraitement linguistique** : nettoyage, lemmatisation, suppression des stopwords pour améliorer la qualité des features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et nettoyage des données\n",
    "\n",
    "On charge les données d'entraînement depuis un fichier JSON. On remplace les valeurs manquantes dans la colonne 'description' par une chaîne vide pour éviter les erreurs lors du traitement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "df = pd.DataFrame(train_data)\n",
    "df['description'] = df['description'].fillna(\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prétraitement linguistique avec SpaCy\n",
    "\n",
    "On utilise SpaCy pour nettoyer et lemmatiser les textes :\n",
    "- Suppression du HTML et des URLs\n",
    "- Mise en minuscules\n",
    "- Lemmatisation (réduction à la racine)\n",
    "- Suppression des stopwords et tokens non alphabétiques\n",
    "\n",
    "Ce prétraitement réduit la dimensionnalité et améliore la qualité des features pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_ for tok in doc if tok.is_alpha and not tok.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['Clean'] = df['description'].apply(clean_and_tokenize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des labels\n",
    "\n",
    "On associe à chaque texte sa catégorie cible pour l'apprentissage supervisé."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "labels_df = pd.read_csv('train_label.csv')\n",
    "df = df.merge(labels_df, on='Id')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorisation des textes avec TF-IDF\n",
    "\n",
    "On transforme les textes en vecteurs de nombres avec `TfidfVectorizer` :\n",
    "- Chaque texte devient un vecteur de taille `max_features` (ici 20 000)\n",
    "- Chaque dimension correspond à un mot ou une paire de mots (n-grammes)\n",
    "- La valeur est le score TF-IDF du mot dans le texte\n",
    "\n",
    "Mathématiquement, le TF-IDF d'un mot $w$ dans un document $d$ est :\n",
    "$$ \\text{tfidf}(w, d) = tf(w, d) \\times \\log\\left(\\frac{N}{df(w)}\\right) $$\n",
    "où $tf(w, d)$ est la fréquence du mot dans le document, $N$ le nombre total de documents, $df(w)$ le nombre de documents contenant $w$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=5, norm='l2')\n",
    "X_tfidf = tfidf.fit_transform(df['Clean'])\n",
    "y = df['Category']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Séparation train/validation\n",
    "\n",
    "On sépare les données en un ensemble d'entraînement (80%) et de validation (20%) pour évaluer la capacité de généralisation du modèle. On stratifie pour conserver la proportion des classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle SVM linéaire\n",
    "\n",
    "Le SVM linéaire cherche l'hyperplan qui maximise la marge entre les classes dans l'espace vectoriel TF-IDF.\n",
    "\n",
    "Mathématiquement, le SVM résout :\n",
    "$$ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 + C \\sum_i \\xi_i $$\n",
    "sous contrainte $y_i (w^T x_i + b) \\geq 1 - \\xi_i$, où $\\xi_i$ sont les variables d'écart (slack variables), $C$ contrôle la régularisation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "svm_clf = LinearSVC(C=1.0, max_iter=5000)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred = svm_clf.predict(X_val)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation du modèle\n",
    "\n",
    "On prédit les classes sur l'ensemble de validation et on affiche un rapport de classification (précision, rappel, F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(classification_report(y_val, y_pred))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prédiction sur les données de test\n",
    "\n",
    "On applique le même prétraitement et la même vectorisation aux textes de test, puis on prédit la catégorie pour chaque texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df['Clean'] = test_df['description'].fillna(\"\").apply(clean_and_tokenize)\n",
    "X_test = tfidf.transform(test_df['Clean'])\n",
    "preds = svm_clf.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Génération du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "template = pd.read_csv('template_submissions.csv')\n",
    "template['Category'] = template['Id'].map(dict(zip(test_df['Id'], preds)))\n",
    "template.to_csv('submission_svm.csv', index=False)\n",
    "print(\"Fichier de soumission enregistré : 'submission_svm.csv'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **TF-IDF** : pondère l'importance des mots selon leur fréquence dans le document et leur rareté dans le corpus.\n",
    "- **SVM linéaire** : cherche l'hyperplan qui sépare au mieux les classes dans l'espace vectoriel, en maximisant la marge.\n",
    "- **Classification** : la sortie du modèle est la classe avec la plus grande distance à l'hyperplan de séparation.\n",
    "\n",
    "Ce pipeline montre comment passer de textes bruts à des prédictions de classes avec un SVM, en utilisant des représentations vectorielles robustes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}