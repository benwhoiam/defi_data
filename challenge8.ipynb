{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte avec TF-IDF et RandomForest\n",
    "\n",
    "Ce notebook détaille un pipeline complet de classification de texte utilisant le prétraitement linguistique avec SpaCy, la vectorisation TF-IDF et un modèle RandomForest. Chaque étape est expliquée, du nettoyage à la génération du fichier de soumission.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts clés\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)** : transforme chaque texte en un vecteur de nombres, reflétant l'importance des mots dans le corpus.\n",
    "- **RandomForestClassifier** : algorithme d'ensemble basé sur la combinaison de plusieurs arbres de décision, robuste et efficace pour la classification.\n",
    "- **Prétraitement linguistique** : nettoyage, lemmatisation, suppression des stopwords pour améliorer la qualité des features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et préparation des données d'entraînement\n",
    "\n",
    "On charge les données d'entraînement depuis un fichier JSON. On remplace les valeurs manquantes dans la colonne 'description' par une chaîne vide pour éviter les erreurs lors du traitement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_train['description'] = df_train['description'].fillna(\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des labels\n",
    "\n",
    "On associe à chaque texte sa catégorie cible pour l'apprentissage supervisé, en fusionnant avec le fichier de labels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "labels_df = pd.read_csv('train_label.csv')  # colonnes : Id, Category\n",
    "df_train = df_train.merge(labels_df, on='Id')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialisation de SpaCy et fonction de nettoyage + lemmatisation\n",
    "\n",
    "On utilise SpaCy pour nettoyer et lemmatiser les textes :\n",
    "- Suppression du HTML et des URLs\n",
    "- Mise en minuscules\n",
    "- Lemmatisation (réduction à la racine)\n",
    "- Suppression des stopwords et tokens non alphabétiques\n",
    "\n",
    "Ce prétraitement réduit la dimensionnalité et améliore la qualité des features pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_ for tok in doc if tok.is_alpha and not tok.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_train['Clean'] = df_train['description'].apply(clean_and_tokenize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorisation TF-IDF sur l'entraînement\n",
    "\n",
    "On transforme les textes en vecteurs de nombres avec `TfidfVectorizer` :\n",
    "- Chaque texte devient un vecteur de taille `max_features` (ici 20 000)\n",
    "- Chaque dimension correspond à un mot ou une paire de mots (n-grammes)\n",
    "- La valeur est le score TF-IDF du mot dans le texte\n",
    "\n",
    "Mathématiquement, le TF-IDF d'un mot $w$ dans un document $d$ est :\n",
    "$$ \\text{tfidf}(w, d) = tf(w, d) \\times \\log\\left(\\frac{N}{df(w)}\\right) $$\n",
    "où $tf(w, d)$ est la fréquence du mot dans le document, $N$ le nombre total de documents, $df(w)$ le nombre de documents contenant $w$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    norm='l2'\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(df_train['Clean'])\n",
    "y = df_train['Category']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split train / validation\n",
    "\n",
    "On sépare les données en un ensemble d'entraînement (80%) et de validation (20%) pour évaluer la capacité de généralisation du modèle. On stratifie pour conserver la proportion des classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tfidf, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle RandomForest\n",
    "\n",
    "Le RandomForest est un ensemble d'arbres de décision. Chaque arbre vote pour une classe, et la classe majoritaire est choisie. Cela permet de réduire le surapprentissage et d'améliorer la robustesse.\n",
    "\n",
    "- `n_estimators=200` : nombre d'arbres\n",
    "- `max_depth=30` : profondeur maximale des arbres\n",
    "- `n_jobs=-1` : utilise tous les cœurs du processeur pour accélérer l'entraînement\n",
    "\n",
    "Mathématiquement, chaque arbre partitionne l'espace des features, et la forêt combine les décisions pour une meilleure généralisation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation sur la validation\n",
    "\n",
    "On prédit les classes sur l'ensemble de validation et on affiche un rapport de classification (précision, rappel, F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_pred = rf_clf.predict(X_val)\n",
    "print(\"=== Rapport de classification sur la validation ===\")\n",
    "print(classification_report(y_val, y_pred))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chargement et préparation des données de test\n",
    "\n",
    "On applique le même nettoyage et la même vectorisation aux textes de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "df_test['description'] = df_test['description'].fillna(\"\")\n",
    "df_test['Clean'] = df_test['description'].apply(clean_and_tokenize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vectorisation TF-IDF sur le test\n",
    "\n",
    "On transforme les textes de test en vecteurs TF-IDF avec le même vocabulaire que l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_test = tfidf.transform(df_test['Clean'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prédiction\n",
    "\n",
    "On prédit la catégorie pour chaque texte de test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "preds = rf_clf.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Préparation du fichier de soumission\n",
    "\n",
    "On prépare le fichier de soumission au format attendu, associant chaque Id de test à la catégorie prédite. On s'assure de l'alignement sur l'Id."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "template = pd.read_csv('template_submissions.csv')  # colonnes : Id, Category\n",
    "mapping = dict(zip(df_test['Id'], preds))\n",
    "template['Category'] = template['Id'].map(mapping)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde\n",
    "\n",
    "On sauvegarde le fichier de soumission au format CSV."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_path = 'submission_rf.csv'\n",
    "template.to_csv(output_path, index=False)\n",
    "print(f\"Fichier de soumission enregistré : '{output_path}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts mathématiques et conclusion\n",
    "\n",
    "- **TF-IDF** : pondère l'importance des mots selon leur fréquence dans le document et leur rareté dans le corpus.\n",
    "- **RandomForest** : combine plusieurs arbres de décision pour améliorer la robustesse et la généralisation.\n",
    "- **Classification** : la sortie du modèle est la classe avec la majorité des votes des arbres.\n",
    "\n",
    "Ce pipeline montre comment passer de textes bruts à des prédictions de classes avec un modèle d'ensemble, en utilisant des représentations vectorielles robustes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}